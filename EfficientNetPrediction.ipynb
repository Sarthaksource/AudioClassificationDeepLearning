{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import torchaudio\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "v6fp7ATLzTYU"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE_RATE = 22050\n",
        "NUM_SAMPLES = 22050\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 128"
      ],
      "metadata": {
        "id": "0rH6aiMu0P6W"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "X8QNlV0FzSCY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class EfficientNetCustom(nn.Module):\n",
        "    def __init__(self, num_classes=50, pretrained=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load EfficientNet-B0\n",
        "        effnet = models.efficientnet_b0(pretrained=pretrained)\n",
        "\n",
        "        # Keep only the convolutional feature extractor (exclude classifier)\n",
        "        self.features = effnet.features\n",
        "\n",
        "        # Modify the first convolutional layer to accept 1 input channel\n",
        "        # EfficientNet-B0's first layer is features[0][0]\n",
        "        original_first_conv = self.features[0][0]\n",
        "        new_first_conv = nn.Conv2d(\n",
        "            1,  # Change input channels from 3 to 1\n",
        "            original_first_conv.out_channels,\n",
        "            kernel_size=original_first_conv.kernel_size,\n",
        "            stride=original_first_conv.stride,\n",
        "            padding=original_first_conv.padding,\n",
        "            bias=original_first_conv.bias\n",
        "        )\n",
        "        # Copy weights from the original first layer (optional, but good practice if applicable)\n",
        "        # For 1 input channel, we can average the weights across the original 3 input channels\n",
        "        new_first_conv.weight.data = original_first_conv.weight.data.mean(dim=1, keepdim=True)\n",
        "\n",
        "        self.features[0][0] = new_first_conv\n",
        "\n",
        "\n",
        "        # Get the output channels of EfficientNet-B0\n",
        "        self.feature_dim = 1280\n",
        "\n",
        "        # Custom fully connected head (your design)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.feature_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features\n",
        "        x = self.features(x)         # shape: [B, 1280, H, W]\n",
        "        x = nn.functional.adaptive_avg_pool2d(x, 1)  # [B, 1280, 1, 1]\n",
        "        x = torch.flatten(x, 1)      # [B, 1280]\n",
        "\n",
        "        # Pass through custom head\n",
        "        out = self.classifier(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "def preprocess_audio(file_path, transformation, target_sample_rate, num_samples, device):\n",
        "    # load audio\n",
        "    signal, sr = torchaudio.load(file_path)\n",
        "    signal = signal.to(device)\n",
        "\n",
        "    # resample\n",
        "    if sr != target_sample_rate:\n",
        "        resampler = torchaudio.transforms.Resample(sr, target_sample_rate).to(device)\n",
        "        signal = resampler(signal)\n",
        "\n",
        "    # mix down to mono\n",
        "    if signal.shape[0] > 1:\n",
        "        signal = torch.mean(signal, dim=0, keepdim=True)\n",
        "\n",
        "    # cut or pad\n",
        "    if signal.shape[1] > num_samples:\n",
        "        signal = signal[:, :num_samples]\n",
        "    elif signal.shape[1] < num_samples:\n",
        "        num_missing = num_samples - signal.shape[1]\n",
        "        signal = F.pad(signal, (0, num_missing))\n",
        "\n",
        "    # apply transformation (e.g., mel spectrogram)\n",
        "    signal = transformation(signal)\n",
        "\n",
        "    return signal\n"
      ],
      "metadata": {
        "id": "bPZK_KBO0Y1-"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_single_audio(model, file_path, transformation, target_sample_rate, num_samples, device, class_mapping=None):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        signal = preprocess_audio(file_path, transformation, target_sample_rate, num_samples, device)\n",
        "        signal = signal.unsqueeze(0).to(device)   # add batch dimension\n",
        "        outputs = model(signal)\n",
        "        predicted_idx = torch.argmax(outputs, dim=1).item()\n",
        "\n",
        "    if class_mapping:\n",
        "        return class_mapping[str(predicted_idx)]\n",
        "    return predicted_idx"
      ],
      "metadata": {
        "collapsed": true,
        "id": "u8Y0OBaG0ewX"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mel_spectrogram = torchaudio.transforms.MelSpectrogram(SAMPLE_RATE, n_fft=1024, hop_length=512, n_mels=64)"
      ],
      "metadata": {
        "id": "LT1fF73R0Wc2"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = EfficientNetCustom(num_classes=50, pretrained=True).to(DEVICE)\n",
        "model.load_state_dict(torch.load(\"/content/audioModelDLNew3.pth\", map_location=DEVICE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1cFN0y64EXd",
        "outputId": "0e1f11f5-3686-4637-881c-04ceb5a9ef55"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"mapping.json\", \"r\") as f:\n",
        "    class_mapping = json.load(f)\n",
        "\n",
        "\n",
        "prediction = predict_single_audio(\n",
        "    model,\n",
        "    \"/content/1-100038-A-14.wav\",\n",
        "    mel_spectrogram,\n",
        "    SAMPLE_RATE,\n",
        "    NUM_SAMPLES,\n",
        "    DEVICE,\n",
        "    class_mapping\n",
        ")\n",
        "\n",
        "print(\"Predicted class:\", prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-tChj0X3Ppb",
        "outputId": "3b9a1480-5912-48a8-9f04-6ee88f99a154"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: chirping_birds\n"
          ]
        }
      ]
    }
  ]
}