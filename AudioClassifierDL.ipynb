{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8X0rH-_u60T",
        "outputId": "2f913115-9ca6-4838-e0d1-aaa9763ed295"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/environmental-sound-classification-50\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"mmoreaux/environmental-sound-classification-50\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biLWbR7fSG1o",
        "outputId": "e78385f3-4ad6-4693-9d73-0654c318ec3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.12/dist-packages (1.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import torchaudio\n",
        "import os\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "4WcUYIKoyuVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ANNOTATION_FILE = \"/kaggle/input/environmental-sound-classification-50/esc50.csv\"\n",
        "AUDIO_DIR = \"/kaggle/input/environmental-sound-classification-50/audio/audio/\"\n",
        "SAMPLE_RATE = 22050\n",
        "NUM_SAMPLES = 22050\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 128"
      ],
      "metadata": {
        "id": "KkVl4HL32wA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, annotation_file, audio_dir, transformation, target_sample_rate, num_samples, device):\n",
        "    self.annotations = pd.read_csv(annotation_file)\n",
        "    self.audio_dir = audio_dir\n",
        "    self.device = device\n",
        "    self.transformation = transformation.to(self.device)\n",
        "    self.target_sample_rate = target_sample_rate\n",
        "    self.num_samples = num_samples\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.annotations)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    audio_sample_path = self._get_audio_sample_path(idx)\n",
        "    label = self._get_audio_sample_label(idx)\n",
        "    signal, sr = torchaudio.load(audio_sample_path)\n",
        "    signal = signal.to(self.device)\n",
        "    # signal -> (num_channels, sr) = (2, 16000) -> (1, 16000)\n",
        "    signal = self._resample(signal, sr)\n",
        "    signal = self._mix_down(signal)\n",
        "    signal = self._cut(signal)\n",
        "    signal = self._right_pad(signal)\n",
        "    signal = self.transformation(signal)\n",
        "\n",
        "    return signal, label\n",
        "\n",
        "  def _cut(self, signal):\n",
        "    if signal.shape[1]>self.num_samples:\n",
        "      signal = signal[:, :self.num_samples]\n",
        "    return signal\n",
        "\n",
        "  def _right_pad(self, signal):\n",
        "    if signal.shape[1]<self.num_samples:\n",
        "      num_missing_samples = self.num_samples - signal.shape[1]\n",
        "      last_dim_padding = (0, num_missing_samples)\n",
        "      signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
        "    return signal\n",
        "\n",
        "  def _resample(self, signal, sample_rate):\n",
        "    if sample_rate != self.target_sample_rate:\n",
        "      resampler = torchaudio.transforms.Resample(sample_rate, self.target_sample_rate).to(self.device)\n",
        "      signal = resampler(signal)\n",
        "    return signal\n",
        "\n",
        "  def _mix_down(self, signal):\n",
        "    if signal.shape[0]>1:\n",
        "      signal = torch.mean(signal, dim=0, keepdim=True)\n",
        "    return signal\n",
        "\n",
        "  def _get_audio_sample_path(self, idx):\n",
        "    path = os.path.join(self.audio_dir, self.annotations.iloc[idx, 0])\n",
        "    return path\n",
        "\n",
        "  def _get_audio_sample_label(self, idx):\n",
        "    return self.annotations.iloc[idx, 2]"
      ],
      "metadata": {
        "id": "-bXmSPXfy1Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mel_spectogram = torchaudio.transforms.MelSpectrogram(SAMPLE_RATE, n_fft=1024, hop_length=512, n_mels=64)"
      ],
      "metadata": {
        "id": "q2WsF2l86MZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "esc50 = CustomDataset(ANNOTATION_FILE, AUDIO_DIR, mel_spectogram, SAMPLE_RATE, NUM_SAMPLES, DEVICE)"
      ],
      "metadata": {
        "id": "b2nARugj2h1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CNNNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Dropout(0.4)\n",
        "        )\n",
        "\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Dropout(0.4)\n",
        "        )\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(128 * 5 * 4, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 50)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        x = self.conv1(input_data)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear(x)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "x6fyUc0aO64g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNNNetwork()\n",
        "model.to(DEVICE)\n",
        "summary(model, (1, 64, 44))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZ_lMd8bSTV6",
        "outputId": "de5ff26c-8585-46fd-c341-7aece81f6527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 66, 46]             160\n",
            "       BatchNorm2d-2           [-1, 16, 66, 46]              32\n",
            "              ReLU-3           [-1, 16, 66, 46]               0\n",
            "         MaxPool2d-4           [-1, 16, 33, 23]               0\n",
            "           Dropout-5           [-1, 16, 33, 23]               0\n",
            "            Conv2d-6           [-1, 32, 35, 25]           4,640\n",
            "       BatchNorm2d-7           [-1, 32, 35, 25]              64\n",
            "              ReLU-8           [-1, 32, 35, 25]               0\n",
            "         MaxPool2d-9           [-1, 32, 17, 12]               0\n",
            "          Dropout-10           [-1, 32, 17, 12]               0\n",
            "           Conv2d-11           [-1, 64, 19, 14]          18,496\n",
            "      BatchNorm2d-12           [-1, 64, 19, 14]             128\n",
            "             ReLU-13           [-1, 64, 19, 14]               0\n",
            "        MaxPool2d-14             [-1, 64, 9, 7]               0\n",
            "          Dropout-15             [-1, 64, 9, 7]               0\n",
            "           Conv2d-16           [-1, 128, 11, 9]          73,856\n",
            "      BatchNorm2d-17           [-1, 128, 11, 9]             256\n",
            "             ReLU-18           [-1, 128, 11, 9]               0\n",
            "        MaxPool2d-19            [-1, 128, 5, 4]               0\n",
            "          Dropout-20            [-1, 128, 5, 4]               0\n",
            "          Flatten-21                 [-1, 2560]               0\n",
            "           Linear-22                  [-1, 256]         655,616\n",
            "             ReLU-23                  [-1, 256]               0\n",
            "          Dropout-24                  [-1, 256]               0\n",
            "           Linear-25                   [-1, 50]          12,850\n",
            "================================================================\n",
            "Total params: 766,098\n",
            "Trainable params: 766,098\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.84\n",
            "Params size (MB): 2.92\n",
            "Estimated Total Size (MB): 5.78\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "esc50\n",
        "train_dataloader = DataLoader(esc50, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "18-NB8EyeRUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "epochs = 50"
      ],
      "metadata": {
        "id": "F4V-yQQueb7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "FNAtBvJneed7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  for X, y in train_dataloader:\n",
        "    X = X.to(DEVICE)\n",
        "    y = y.to(DEVICE)\n",
        "\n",
        "    output = model(X)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss = loss_fn(output, y)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "  print(f\"Epoch: {epoch} | Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHgTOffTeh0Q",
        "outputId": "3bd1b85d-8ebd-48c5-f463-0b7293c0fe24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 2.612142562866211\n",
            "Epoch: 1 | Loss: 2.4953036308288574\n",
            "Epoch: 2 | Loss: 2.4880001544952393\n",
            "Epoch: 3 | Loss: 2.3570199012756348\n",
            "Epoch: 4 | Loss: 2.2784740924835205\n",
            "Epoch: 5 | Loss: 2.2443394660949707\n",
            "Epoch: 6 | Loss: 2.2194249629974365\n",
            "Epoch: 7 | Loss: 2.2883286476135254\n",
            "Epoch: 8 | Loss: 2.355990409851074\n",
            "Epoch: 9 | Loss: 2.055131196975708\n",
            "Epoch: 10 | Loss: 2.056495189666748\n",
            "Epoch: 11 | Loss: 1.8995975255966187\n",
            "Epoch: 12 | Loss: 2.0249929428100586\n",
            "Epoch: 13 | Loss: 1.8518203496932983\n",
            "Epoch: 14 | Loss: 1.9143178462982178\n",
            "Epoch: 15 | Loss: 1.8892837762832642\n",
            "Epoch: 16 | Loss: 1.8667023181915283\n",
            "Epoch: 17 | Loss: 1.8538926839828491\n",
            "Epoch: 18 | Loss: 1.9100711345672607\n",
            "Epoch: 19 | Loss: 1.7310912609100342\n",
            "Epoch: 20 | Loss: 1.6848903894424438\n",
            "Epoch: 21 | Loss: 1.7430782318115234\n",
            "Epoch: 22 | Loss: 1.9179519414901733\n",
            "Epoch: 23 | Loss: 1.6599435806274414\n",
            "Epoch: 24 | Loss: 1.4533237218856812\n",
            "Epoch: 25 | Loss: 1.689745545387268\n",
            "Epoch: 26 | Loss: 1.7405128479003906\n",
            "Epoch: 27 | Loss: 1.618612289428711\n",
            "Epoch: 28 | Loss: 1.389373540878296\n",
            "Epoch: 29 | Loss: 1.6349595785140991\n",
            "Epoch: 30 | Loss: 1.656134009361267\n",
            "Epoch: 31 | Loss: 1.554579496383667\n",
            "Epoch: 32 | Loss: 1.5179082155227661\n",
            "Epoch: 33 | Loss: 1.4349068403244019\n",
            "Epoch: 34 | Loss: 1.2611452341079712\n",
            "Epoch: 35 | Loss: 1.4310996532440186\n",
            "Epoch: 36 | Loss: 1.2672462463378906\n",
            "Epoch: 37 | Loss: 1.287356972694397\n",
            "Epoch: 38 | Loss: 1.390183687210083\n",
            "Epoch: 39 | Loss: 1.4153903722763062\n",
            "Epoch: 40 | Loss: 1.2495313882827759\n",
            "Epoch: 41 | Loss: 1.3909623622894287\n",
            "Epoch: 42 | Loss: 1.1106035709381104\n",
            "Epoch: 43 | Loss: 1.093634009361267\n",
            "Epoch: 44 | Loss: 1.3656612634658813\n",
            "Epoch: 45 | Loss: 1.271571397781372\n",
            "Epoch: 46 | Loss: 1.3544187545776367\n",
            "Epoch: 47 | Loss: 1.384186029434204\n",
            "Epoch: 48 | Loss: 1.2074989080429077\n",
            "Epoch: 49 | Loss: 1.347800374031067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"audioModelDL.pth\")"
      ],
      "metadata": {
        "id": "9J3JD4i4ekNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.read_csv(ANNOTATION_FILE)\n",
        "# unique_mapping_df = df.drop_duplicates(subset=['target', 'category'])\n",
        "\n",
        "# mapping_series = unique_mapping_df.set_index('target')['category']\n",
        "\n",
        "# mapping_dict = mapping_series.to_dict()\n",
        "\n",
        "# print(mapping_dict)"
      ],
      "metadata": {
        "id": "v0CWrnGCzVsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "\n",
        "# with open(\"mapping.json\", \"w\") as f:\n",
        "#   json.dump(mapping_dict, f)"
      ],
      "metadata": {
        "id": "mCaY0aopziwM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}